services:
  # Frigate NVR
  frigate:
    container_name: frigate
    image: ghcr.io/blakeblackshear/frigate:stable-tensorrt
    privileged: true
    restart: unless-stopped
    shm_size: "2g"  # Increased for better streaming performance
    # Use host networking for WebRTC connectivity
    network_mode: "host"
    # devices:
    #   - /dev/dri:/dev/dri  # Intel QSV hardware acceleration (removed; using NVIDIA)
    volumes:
      - ./frigate/config:/config
      - ./frigate/config/go2rtc:/usr/local/go2rtc/bin/go2rtc:ro
      - ./frigate/media:/media/frigate
      - /etc/localtime:/etc/localtime:ro
    # No ports needed with host networking
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']
              capabilities: [gpu, compute, video, graphics, utility]  # Full capabilities for NVENC/NVDEC
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,video,utility,graphics
      - YOLO_MODELS=yolov7-320
      - USE_FP16=true
      - TRT_MODEL_PREP_DEVICE=0
      - CUDA_VISIBLE_DEVICES=0
      # Hardware acceleration environment variables (Intel VAAPI removed)
      # - LIBVA_DRIVER_NAME=i965
      # - VAAPI_DEVICE=/dev/dri/renderD128
    # Host networking - no depends_on or networks needed
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  # MQTT Broker
  mosquitto:
    container_name: mosquitto
    image: eclipse-mosquitto:2.0
    restart: unless-stopped
    volumes:
      - ./mosquitto/config:/mosquitto/config
      - ./mosquitto/data:/mosquitto/data
      - ./mosquitto/log:/mosquitto/log
    ports:
      - "1883:1883"
      - "9001:9001"
    networks:
      - anchorpoint-network

  # PWA Backend (Flask)
  pwa-backend:
    container_name: pwa-backend
    build:
      context: ./pwa/backend
      dockerfile: Dockerfile
    restart: unless-stopped
    volumes:
      - ./data:/data
      - ./pwa/backend:/app
      - ./frigate/media:/media/frigate
      - ./hls-segments:/segments
    ports:
      - "5003:5003"   # Backend API
    env_file:
      - ./pwa/backend/config.env
    environment:
      - FLASK_ENV=development
      - SECRET_KEY=${SECRET_KEY:-your-secret-key-change-in-production}
      - DATABASE_PATH=/data/anchorpoint.db
      - FRIGATE_HOST=http://172.18.0.1:5000
      - GO2RTC_HOST=http://172.18.0.1:1984
      - HLS_SEGMENTS_PATH=/segments
      - FFMPEG_SERVICE_HOST=http://172.18.0.1:8080
    depends_on:
      - frigate
    networks:
      - anchorpoint-network
    healthcheck:
      test: ["CMD", "python", "-c", "import requests; requests.get('http://localhost:5003/health')"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # PWA Frontend (Vue.js)
  pwa-frontend:
    container_name: pwa-frontend
    image: node:20
    working_dir: /app
    volumes:
      - ./pwa:/app
    # No ports exposed to host; Caddy reverse proxy handles all external access
    command: sh -c "npm install && npm run dev -- --host=0.0.0.0"
    depends_on:
      - pwa-backend
    networks:
      - anchorpoint-network

  # Caddy Reverse Proxy (for production)
  caddy:
    container_name: caddy
    image: caddy:2-alpine
    restart: unless-stopped
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./caddy/Caddyfile:/etc/caddy/Caddyfile
      - ./caddy/data:/data
      - ./caddy/config:/config
    depends_on:
      - pwa-backend
      - pwa-frontend
    networks:
      - anchorpoint-network



  # FFmpeg container for RTSP to HLS conversion
  ffmpeg-streamer:
    container_name: ffmpeg-streamer
    build:
      context: ./ffmpeg
      dockerfile: Dockerfile
    restart: unless-stopped
    network_mode: "host"  # Use host networking to access cameras
    env_file:
      - ./pwa/backend/config.env
    volumes:
      - ./hls-segments:/segments
    # Custom Ubuntu image with FFmpeg and our streaming script

networks:
  anchorpoint-network:
    driver: bridge 